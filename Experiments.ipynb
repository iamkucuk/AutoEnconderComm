{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, GaussianNoise, Lambda, Dropout, concatenate, LSTM, Add, Multiply, Layer\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from keras.utils import plot_model\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = K.constant([[1, 1]])\n",
    "SNR1 = 7\n",
    "SNR2 = 7\n",
    "SNRs = [SNR1, SNR2]\n",
    "ebno = [calc_ebno(SNR) for SNR in SNRs]\n",
    "\n",
    "k = 2\n",
    "n_channel = 2\n",
    "M = 2 ** k\n",
    "k = int(k)\n",
    "R = k / n_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransmissionLayer(x, t, k):\n",
    "    signal = H[t, k] * x\n",
    "\n",
    "    for i in range(t):\n",
    "        if i == k:\n",
    "            continue\n",
    "        interference = H[i, k] * x\n",
    "        signal = signal + interference\n",
    "\n",
    "\n",
    "#     noise = K.random_normal(K.shape(signal),\n",
    "#                         mean=0,\n",
    "#                         stddev=np.sqrt( 1/ (2 * R * ebno[k])))\n",
    "    return signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaCallback(Callback):\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        loss1, loss2 = logs[\"decoder1_loss\"], logs[\"decoder2_loss\"]\n",
    "        K.set_value(self.alpha, loss1 / (loss1 + loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Furkan\\miniconda3\\envs\\commtf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Furkan\\miniconda3\\envs\\commtf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_signal1 = Input(shape=(M,), name=\"input1\")\n",
    "input_signal2 = Input(shape=(M,), name=\"input2\")\n",
    "encoder1 = Sequential([\n",
    "    Dense(M, activation=\"relu\"),\n",
    "    Dense(n_channel, activation=\"linear\")\n",
    "], name=\"encoder1\")\n",
    "encoder2 = Sequential([\n",
    "    Dense(M, activation=\"relu\"),\n",
    "    Dense(n_channel, activation=\"linear\")\n",
    "], name=\"encoder2\")\n",
    "\n",
    "combiner = Sequential([\n",
    "    Dense(n_channel, activation=\"relu\"),\n",
    "    Dense(n_channel, activation=\"linear\"),\n",
    "    BatchNormalization(center=False, scale=False),\n",
    "], name=\"combiner\")\n",
    "\n",
    "signal_input1 = Sequential(\n",
    "    [Lambda(TransmissionLayer, arguments={\"t\":0, \"k\":0}),\n",
    "    GaussianNoise(np.sqrt(1 / (2 * R * ebno[0])))], name=\"transmit1\")\n",
    "\n",
    "signal_input2 = Sequential(\n",
    "    [Lambda(TransmissionLayer, arguments={\"t\":0, \"k\":1}),\n",
    "    GaussianNoise(np.sqrt(1 / (2 * R * ebno[1])))], name=\"transmit2\")\n",
    "\n",
    "decoder1 = Sequential([\n",
    "    Dense(M, activation=\"relu\"),\n",
    "    Dense(M, activation=\"relu\"),\n",
    "    Dense(M, activation=\"softmax\")\n",
    "], name=\"decoder1\")\n",
    "\n",
    "decoder2 = Sequential([\n",
    "    Dense(M, activation=\"relu\"),\n",
    "    Dense(M, activation=\"relu\"),\n",
    "    Dense(M, activation=\"softmax\")\n",
    "], name=\"decoder2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Furkan\\miniconda3\\envs\\commtf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Furkan\\miniconda3\\envs\\commtf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Furkan\\miniconda3\\envs\\commtf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input1 (InputLayer)             (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input2 (InputLayer)             (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder1 (Sequential)           (None, 2)            30          input1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder2 (Sequential)           (None, 2)            30          input2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4)            0           encoder1[1][0]                   \n",
      "                                                                 encoder2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "combiner (Sequential)           (None, 2)            20          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "transmit1 (Sequential)          (None, 2)            0           combiner[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "transmit2 (Sequential)          (None, 2)            0           combiner[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decoder1 (Sequential)           (None, 4)            52          transmit1[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder2 (Sequential)           (None, 4)            52          transmit2[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 184\n",
      "Trainable params: 180\n",
      "Non-trainable params: 4\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Furkan\\miniconda3\\envs\\commtf\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Furkan\\miniconda3\\envs\\commtf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x1 = encoder1(input_signal1)\n",
    "x2 = encoder2(input_signal2)\n",
    "x = concatenate([x1, x2], axis=1)\n",
    "x = combiner(x)\n",
    "x1 = signal_input1(x)\n",
    "x2 = signal_input2(x)\n",
    "out1 = decoder1(x1)\n",
    "out2 = decoder2(x2)\n",
    "\n",
    "model = Model(inputs=[input_signal1, input_signal2], outputs=[out1, out2])\n",
    "model.summary()\n",
    "alpha = K.variable(.5)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=[\"categorical_crossentropy\", \"categorical_crossentropy\"],\n",
    "            loss_weights=[alpha, (1 - alpha)], metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 200000 samples\n",
      "Epoch 1/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 1.0439 - decoder1_loss: 1.0249 - decoder2_loss: 1.0622 - decoder1_acc: 0.4834 - decoder2_acc: 0.4473 - val_loss: 1.1115 - val_decoder1_loss: 1.1281 - val_decoder2_loss: 1.0953 - val_decoder1_acc: 0.3757 - val_decoder2_acc: 0.4373\n",
      "Epoch 2/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 1.0196 - decoder1_loss: 1.0009 - decoder2_loss: 1.0377 - decoder1_acc: 0.4854 - decoder2_acc: 0.5475 - val_loss: 1.0824 - val_decoder1_loss: 1.1025 - val_decoder2_loss: 1.0633 - val_decoder1_acc: 0.3757 - val_decoder2_acc: 0.6252\n",
      "Epoch 3/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.9982 - decoder1_loss: 0.9815 - decoder2_loss: 1.0143 - decoder1_acc: 0.4897 - decoder2_acc: 0.5687 - val_loss: 1.0444 - val_decoder1_loss: 1.0602 - val_decoder2_loss: 1.0292 - val_decoder1_acc: 0.3757 - val_decoder2_acc: 0.6252\n",
      "Epoch 4/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.9787 - decoder1_loss: 0.9639 - decoder2_loss: 0.9931 - decoder1_acc: 0.4963 - decoder2_acc: 0.5783 - val_loss: 1.0135 - val_decoder1_loss: 1.0252 - val_decoder2_loss: 1.0020 - val_decoder1_acc: 0.3757 - val_decoder2_acc: 0.6252\n",
      "Epoch 5/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.9614 - decoder1_loss: 0.9481 - decoder2_loss: 0.9744 - decoder1_acc: 0.5027 - decoder2_acc: 0.5906 - val_loss: 0.9823 - val_decoder1_loss: 0.9907 - val_decoder2_loss: 0.9742 - val_decoder1_acc: 0.4370 - val_decoder2_acc: 0.6252\n",
      "Epoch 6/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.9447 - decoder1_loss: 0.9341 - decoder2_loss: 0.9551 - decoder1_acc: 0.5099 - decoder2_acc: 0.6002 - val_loss: 0.9618 - val_decoder1_loss: 0.9711 - val_decoder2_loss: 0.9527 - val_decoder1_acc: 0.4993 - val_decoder2_acc: 0.6252\n",
      "Epoch 7/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.9299 - decoder1_loss: 0.9220 - decoder2_loss: 0.9377 - decoder1_acc: 0.5181 - decoder2_acc: 0.6086 - val_loss: 0.9513 - val_decoder1_loss: 0.9667 - val_decoder2_loss: 0.9361 - val_decoder1_acc: 0.4993 - val_decoder2_acc: 0.6252\n",
      "Epoch 8/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.9166 - decoder1_loss: 0.9115 - decoder2_loss: 0.9216 - decoder1_acc: 0.5248 - decoder2_acc: 0.6156 - val_loss: 0.9233 - val_decoder1_loss: 0.9340 - val_decoder2_loss: 0.9127 - val_decoder1_acc: 0.4993 - val_decoder2_acc: 0.6252\n",
      "Epoch 9/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.9032 - decoder1_loss: 0.9004 - decoder2_loss: 0.9060 - decoder1_acc: 0.5343 - decoder2_acc: 0.6232 - val_loss: 0.9055 - val_decoder1_loss: 0.9179 - val_decoder2_loss: 0.8935 - val_decoder1_acc: 0.4993 - val_decoder2_acc: 0.6252\n",
      "Epoch 10/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.8909 - decoder1_loss: 0.8902 - decoder2_loss: 0.8916 - decoder1_acc: 0.5455 - decoder2_acc: 0.6272 - val_loss: 0.8971 - val_decoder1_loss: 0.9134 - val_decoder2_loss: 0.8808 - val_decoder1_acc: 0.4368 - val_decoder2_acc: 0.6252\n",
      "Epoch 11/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.8779 - decoder1_loss: 0.8774 - decoder2_loss: 0.8784 - decoder1_acc: 0.5586 - decoder2_acc: 0.6313 - val_loss: 0.8797 - val_decoder1_loss: 0.8956 - val_decoder2_loss: 0.8638 - val_decoder1_acc: 0.4368 - val_decoder2_acc: 0.6252\n",
      "Epoch 12/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.8656 - decoder1_loss: 0.8636 - decoder2_loss: 0.8676 - decoder1_acc: 0.5709 - decoder2_acc: 0.6326 - val_loss: 0.8672 - val_decoder1_loss: 0.8832 - val_decoder2_loss: 0.8512 - val_decoder1_acc: 0.3749 - val_decoder2_acc: 0.6875\n",
      "Epoch 13/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.8525 - decoder1_loss: 0.8469 - decoder2_loss: 0.8581 - decoder1_acc: 0.5849 - decoder2_acc: 0.6328 - val_loss: 0.8466 - val_decoder1_loss: 0.8552 - val_decoder2_loss: 0.8380 - val_decoder1_acc: 0.3749 - val_decoder2_acc: 0.6875\n",
      "Epoch 14/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.8390 - decoder1_loss: 0.8271 - decoder2_loss: 0.8505 - decoder1_acc: 0.5981 - decoder2_acc: 0.6314 - val_loss: 0.8094 - val_decoder1_loss: 0.7996 - val_decoder2_loss: 0.8189 - val_decoder1_acc: 0.4374 - val_decoder2_acc: 0.6875\n",
      "Epoch 15/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.8264 - decoder1_loss: 0.8063 - decoder2_loss: 0.8456 - decoder1_acc: 0.6085 - decoder2_acc: 0.6277 - val_loss: 0.8153 - val_decoder1_loss: 0.8060 - val_decoder2_loss: 0.8240 - val_decoder1_acc: 0.4374 - val_decoder2_acc: 0.6252\n",
      "Epoch 16/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.8144 - decoder1_loss: 0.7841 - decoder2_loss: 0.8426 - decoder1_acc: 0.6178 - decoder2_acc: 0.6239 - val_loss: 0.7983 - val_decoder1_loss: 0.7805 - val_decoder2_loss: 0.8145 - val_decoder1_acc: 0.4374 - val_decoder2_acc: 0.6252\n",
      "Epoch 17/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.8038 - decoder1_loss: 0.7625 - decoder2_loss: 0.8412 - decoder1_acc: 0.6269 - decoder2_acc: 0.6187 - val_loss: 0.7769 - val_decoder1_loss: 0.7437 - val_decoder2_loss: 0.8062 - val_decoder1_acc: 0.4374 - val_decoder2_acc: 0.6252\n",
      "Epoch 18/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.7950 - decoder1_loss: 0.7408 - decoder2_loss: 0.8426 - decoder1_acc: 0.6350 - decoder2_acc: 0.6114 - val_loss: 0.7577 - val_decoder1_loss: 0.7079 - val_decoder2_loss: 0.8012 - val_decoder1_acc: 0.6245 - val_decoder2_acc: 0.6252\n",
      "Epoch 19/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.7861 - decoder1_loss: 0.7128 - decoder2_loss: 0.8478 - decoder1_acc: 0.6442 - decoder2_acc: 0.6018 - val_loss: 0.7367 - val_decoder1_loss: 0.6591 - val_decoder2_loss: 0.7999 - val_decoder1_acc: 0.6868 - val_decoder2_acc: 0.6252\n",
      "Epoch 20/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.7773 - decoder1_loss: 0.6814 - decoder2_loss: 0.8539 - decoder1_acc: 0.6601 - decoder2_acc: 0.5955 - val_loss: 0.7147 - val_decoder1_loss: 0.6055 - val_decoder2_loss: 0.8000 - val_decoder1_acc: 0.6248 - val_decoder2_acc: 0.6252\n",
      "Epoch 21/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.7689 - decoder1_loss: 0.6496 - decoder2_loss: 0.8593 - decoder1_acc: 0.6752 - decoder2_acc: 0.5906 - val_loss: 0.7095 - val_decoder1_loss: 0.5804 - val_decoder2_loss: 0.8060 - val_decoder1_acc: 0.6866 - val_decoder2_acc: 0.6252\n",
      "Epoch 22/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.7574 - decoder1_loss: 0.6211 - decoder2_loss: 0.8564 - decoder1_acc: 0.6851 - decoder2_acc: 0.5916 - val_loss: 0.7109 - val_decoder1_loss: 0.5693 - val_decoder2_loss: 0.8119 - val_decoder1_acc: 0.6864 - val_decoder2_acc: 0.6252\n",
      "Epoch 23/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.7447 - decoder1_loss: 0.6009 - decoder2_loss: 0.8467 - decoder1_acc: 0.6923 - decoder2_acc: 0.5935 - val_loss: 0.7196 - val_decoder1_loss: 0.5789 - val_decoder2_loss: 0.8213 - val_decoder1_acc: 0.6240 - val_decoder2_acc: 0.6252\n",
      "Epoch 24/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.7303 - decoder1_loss: 0.5844 - decoder2_loss: 0.8328 - decoder1_acc: 0.6975 - decoder2_acc: 0.5975 - val_loss: 0.7383 - val_decoder1_loss: 0.6028 - val_decoder2_loss: 0.8334 - val_decoder1_acc: 0.6866 - val_decoder2_acc: 0.6252\n",
      "Epoch 25/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.7143 - decoder1_loss: 0.5713 - decoder2_loss: 0.8145 - decoder1_acc: 0.7022 - decoder2_acc: 0.6018 - val_loss: 0.8046 - val_decoder1_loss: 0.7236 - val_decoder2_loss: 0.8615 - val_decoder1_acc: 0.6244 - val_decoder2_acc: 0.6252\n",
      "Epoch 26/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.7046 - decoder1_loss: 0.5651 - decoder2_loss: 0.8029 - decoder1_acc: 0.6972 - decoder2_acc: 0.6032 - val_loss: 0.7824 - val_decoder1_loss: 0.7011 - val_decoder2_loss: 0.8387 - val_decoder1_acc: 0.6244 - val_decoder2_acc: 0.6252\n",
      "Epoch 27/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6962 - decoder1_loss: 0.5552 - decoder2_loss: 0.7947 - decoder1_acc: 0.7003 - decoder2_acc: 0.6052 - val_loss: 0.7512 - val_decoder1_loss: 0.6615 - val_decoder2_loss: 0.8132 - val_decoder1_acc: 0.6244 - val_decoder2_acc: 0.6252\n",
      "Epoch 28/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6892 - decoder1_loss: 0.5473 - decoder2_loss: 0.7878 - decoder1_acc: 0.7015 - decoder2_acc: 0.6065 - val_loss: 0.7195 - val_decoder1_loss: 0.6157 - val_decoder2_loss: 0.7917 - val_decoder1_acc: 0.6244 - val_decoder2_acc: 0.6252\n",
      "Epoch 29/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6825 - decoder1_loss: 0.5384 - decoder2_loss: 0.7817 - decoder1_acc: 0.7042 - decoder2_acc: 0.6071 - val_loss: 0.7134 - val_decoder1_loss: 0.6182 - val_decoder2_loss: 0.7798 - val_decoder1_acc: 0.5621 - val_decoder2_acc: 0.6252\n",
      "Epoch 30/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6773 - decoder1_loss: 0.5327 - decoder2_loss: 0.7766 - decoder1_acc: 0.7062 - decoder2_acc: 0.6085 - val_loss: 0.6862 - val_decoder1_loss: 0.5767 - val_decoder2_loss: 0.7619 - val_decoder1_acc: 0.6244 - val_decoder2_acc: 0.6252\n",
      "Epoch 31/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6717 - decoder1_loss: 0.5258 - decoder2_loss: 0.7713 - decoder1_acc: 0.7067 - decoder2_acc: 0.6090 - val_loss: 0.6583 - val_decoder1_loss: 0.5294 - val_decoder2_loss: 0.7452 - val_decoder1_acc: 0.6866 - val_decoder2_acc: 0.6252\n",
      "Epoch 32/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6676 - decoder1_loss: 0.5208 - decoder2_loss: 0.7672 - decoder1_acc: 0.7076 - decoder2_acc: 0.6096 - val_loss: 0.6341 - val_decoder1_loss: 0.4889 - val_decoder2_loss: 0.7321 - val_decoder1_acc: 0.7493 - val_decoder2_acc: 0.6252\n",
      "Epoch 33/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6626 - decoder1_loss: 0.5145 - decoder2_loss: 0.7626 - decoder1_acc: 0.7095 - decoder2_acc: 0.6104 - val_loss: 0.6130 - val_decoder1_loss: 0.4555 - val_decoder2_loss: 0.7194 - val_decoder1_acc: 0.7490 - val_decoder2_acc: 0.6252\n",
      "Epoch 34/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6580 - decoder1_loss: 0.5088 - decoder2_loss: 0.7582 - decoder1_acc: 0.7102 - decoder2_acc: 0.6106 - val_loss: 0.5989 - val_decoder1_loss: 0.4333 - val_decoder2_loss: 0.7112 - val_decoder1_acc: 0.7490 - val_decoder2_acc: 0.6252\n",
      "Epoch 35/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6541 - decoder1_loss: 0.5027 - decoder2_loss: 0.7550 - decoder1_acc: 0.7120 - decoder2_acc: 0.6113 - val_loss: 0.5911 - val_decoder1_loss: 0.4213 - val_decoder2_loss: 0.7050 - val_decoder1_acc: 0.7490 - val_decoder2_acc: 0.6252\n",
      "Epoch 36/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6500 - decoder1_loss: 0.4989 - decoder2_loss: 0.7505 - decoder1_acc: 0.7130 - decoder2_acc: 0.6122 - val_loss: 0.5894 - val_decoder1_loss: 0.4219 - val_decoder2_loss: 0.7011 - val_decoder1_acc: 0.7496 - val_decoder2_acc: 0.6252\n",
      "Epoch 37/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6465 - decoder1_loss: 0.4950 - decoder2_loss: 0.7469 - decoder1_acc: 0.7131 - decoder2_acc: 0.6120 - val_loss: 0.5886 - val_decoder1_loss: 0.4229 - val_decoder2_loss: 0.6986 - val_decoder1_acc: 0.7496 - val_decoder2_acc: 0.6252\n",
      "Epoch 38/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6428 - decoder1_loss: 0.4903 - decoder2_loss: 0.7434 - decoder1_acc: 0.7127 - decoder2_acc: 0.6103 - val_loss: 0.5864 - val_decoder1_loss: 0.4198 - val_decoder2_loss: 0.6953 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.6252\n",
      "Epoch 39/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6398 - decoder1_loss: 0.4884 - decoder2_loss: 0.7396 - decoder1_acc: 0.7133 - decoder2_acc: 0.6121 - val_loss: 0.5788 - val_decoder1_loss: 0.4113 - val_decoder2_loss: 0.6912 - val_decoder1_acc: 0.7496 - val_decoder2_acc: 0.6252\n",
      "Epoch 40/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6368 - decoder1_loss: 0.4857 - decoder2_loss: 0.7366 - decoder1_acc: 0.7113 - decoder2_acc: 0.6169 - val_loss: 0.5715 - val_decoder1_loss: 0.3985 - val_decoder2_loss: 0.6857 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.6252\n",
      "Epoch 41/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6327 - decoder1_loss: 0.4809 - decoder2_loss: 0.7325 - decoder1_acc: 0.7155 - decoder2_acc: 0.6208 - val_loss: 0.5706 - val_decoder1_loss: 0.3974 - val_decoder2_loss: 0.6822 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.6252\n",
      "Epoch 42/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6295 - decoder1_loss: 0.4795 - decoder2_loss: 0.7281 - decoder1_acc: 0.7140 - decoder2_acc: 0.6247 - val_loss: 0.5654 - val_decoder1_loss: 0.3960 - val_decoder2_loss: 0.6783 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.6252\n",
      "Epoch 43/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6254 - decoder1_loss: 0.4768 - decoder2_loss: 0.7234 - decoder1_acc: 0.7140 - decoder2_acc: 0.6295 - val_loss: 0.5619 - val_decoder1_loss: 0.3930 - val_decoder2_loss: 0.6741 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.6252\n",
      "Epoch 44/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6207 - decoder1_loss: 0.4743 - decoder2_loss: 0.7176 - decoder1_acc: 0.7159 - decoder2_acc: 0.6365 - val_loss: 0.5525 - val_decoder1_loss: 0.3850 - val_decoder2_loss: 0.6606 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.6252\n",
      "Epoch 45/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6168 - decoder1_loss: 0.4721 - decoder2_loss: 0.7126 - decoder1_acc: 0.7147 - decoder2_acc: 0.6411 - val_loss: 0.5448 - val_decoder1_loss: 0.3855 - val_decoder2_loss: 0.6508 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.7507\n",
      "Epoch 46/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6124 - decoder1_loss: 0.4714 - decoder2_loss: 0.7064 - decoder1_acc: 0.7142 - decoder2_acc: 0.6515 - val_loss: 0.5308 - val_decoder1_loss: 0.3784 - val_decoder2_loss: 0.6335 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.7507\n",
      "Epoch 47/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6075 - decoder1_loss: 0.4706 - decoder2_loss: 0.6998 - decoder1_acc: 0.7145 - decoder2_acc: 0.6594 - val_loss: 0.5211 - val_decoder1_loss: 0.3719 - val_decoder2_loss: 0.6174 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.6252\n",
      "Epoch 48/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.6025 - decoder1_loss: 0.4701 - decoder2_loss: 0.6921 - decoder1_acc: 0.7148 - decoder2_acc: 0.6671 - val_loss: 0.5070 - val_decoder1_loss: 0.3688 - val_decoder2_loss: 0.6009 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.6252\n",
      "Epoch 49/2000\n",
      "200000/200000 [==============================] - 0s 2us/step - loss: 0.5969 - decoder1_loss: 0.4692 - decoder2_loss: 0.6842 - decoder1_acc: 0.7141 - decoder2_acc: 0.6751 - val_loss: 0.4989 - val_decoder1_loss: 0.3686 - val_decoder2_loss: 0.5915 - val_decoder1_acc: 0.7495 - val_decoder2_acc: 0.6252\n",
      "Epoch 50/2000\n",
      " 10000/200000 [>.............................] - ETA: 0s - loss: 0.5966 - decoder1_loss: 0.4842 - decoder2_loss: 0.6764 - decoder1_acc: 0.7094 - decoder2_acc: 0.6775"
     ]
    }
   ],
   "source": [
    "train_datas = generate_train_datas()\n",
    "\n",
    "model.fit(x=train_datas, y=train_datas, validation_split=.5, batch_size=10000,\n",
    "    epochs=2000, callbacks=[\n",
    "    EarlyStopping(patience=100, restore_best_weights=True, monitor=\"loss\"),\n",
    "    ReduceLROnPlateau(monitor=\"loss\", factor=.5, patience=20),\n",
    "    AlphaCallback(alpha)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('commtf': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596491713875"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}