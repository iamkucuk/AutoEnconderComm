{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing libs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, GaussianNoise, Lambda\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "import random as rn\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def frange(x, y, jump):\n",
    "  while x < y:\n",
    "    yield x\n",
    "    x += jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 256 k: 8 n:  8 R:  1.0\n"
     ]
    }
   ],
   "source": [
    "# defining parameters\n",
    "k = 8\n",
    "M = 2**k\n",
    "# k = np.log2(M)\n",
    "# k = int(k)\n",
    "n_channel = 8\n",
    "R = k/n_channel\n",
    "print ('M:',M,'k:',k, \"n: \", n_channel, \"R: \", R)\n",
    "\n",
    "EbNodB_range = list(frange(-4,8,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=24\n",
    "tf.set_random_seed(seed)\n",
    "rn.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating data of size N\n",
    "N = 10000\n",
    "label = np.random.randint(M,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating one hot encoded vectors\n",
    "data = []\n",
    "for i in label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 256)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "135 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "217 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "233 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "199 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "42 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "194 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "101 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "141 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "temp_check = [17,23,45,67,89,96,72,250,350]\n",
    "for i in temp_check:\n",
    "    print(label[i],data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (int(k/R))\n",
    "input_signal = Input(shape=(M,))\n",
    "encoded = Dense(M, activation='relu')(input_signal)\n",
    "encoded1 = Dense(n_channel, activation='linear')(encoded)\n",
    "encoded2 = BatchNormalization(center=False, scale=False)(encoded1)\n",
    "# encoded2 = Lambda(lambda x: 1.0/np.sqrt(2)*K.l2_normalize(x,axis=-1))(encoded2)\n",
    "\n",
    "EbNo_train = 5.01187 #  coverted 7 db of EbNo\n",
    "encoded3 = GaussianNoise(np.sqrt(1/(2*R*EbNo_train)))(encoded2)\n",
    "\n",
    "decoded = Dense(M, activation='relu')(encoded3)\n",
    "decoded1 = Dense(M, activation='softmax')(decoded)\n",
    "\n",
    "autoencoder = Model(input_signal, decoded1)\n",
    "#sgd = SGD(lr=0.001)\n",
    "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_48 (InputLayer)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 8)                 2056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 8)                 16        \n",
      "_________________________________________________________________\n",
      "gaussian_noise_24 (GaussianN (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 256)               65792     \n",
      "=================================================================\n",
      "Total params: 135,960\n",
      "Trainable params: 135,944\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_val = 1500\n",
    "val_label = np.random.randint(M,size=N_val)\n",
    "val_data = []\n",
    "for i in val_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    val_data.append(temp)\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1500 samples\n",
      "Epoch 1/1000\n",
      "10000/10000 [==============================] - 1s 125us/step - loss: 5.0234 - val_loss: 5.3825\n",
      "Epoch 2/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 3.8398 - val_loss: 4.8158\n",
      "Epoch 3/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 2.5010 - val_loss: 3.7998\n",
      "Epoch 4/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.4425 - val_loss: 2.6155\n",
      "Epoch 5/1000\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.8196 - val_loss: 1.5723\n",
      "Epoch 6/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.4907 - val_loss: 0.8322\n",
      "Epoch 7/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.3237 - val_loss: 0.4107\n",
      "Epoch 8/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.2417 - val_loss: 0.2006\n",
      "Epoch 9/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.1818 - val_loss: 0.1018\n",
      "Epoch 10/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.1450 - val_loss: 0.0556\n",
      "Epoch 11/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.1192 - val_loss: 0.0335\n",
      "Epoch 12/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.1060 - val_loss: 0.0215\n",
      "Epoch 13/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0894 - val_loss: 0.0147\n",
      "Epoch 14/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0819 - val_loss: 0.0108\n",
      "Epoch 15/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0734 - val_loss: 0.0082\n",
      "Epoch 16/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0664 - val_loss: 0.0064\n",
      "Epoch 17/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0596 - val_loss: 0.0052\n",
      "Epoch 18/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0565 - val_loss: 0.0042\n",
      "Epoch 19/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0533 - val_loss: 0.0035\n",
      "Epoch 20/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0464 - val_loss: 0.0030\n",
      "Epoch 21/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0448 - val_loss: 0.0025\n",
      "Epoch 22/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0420 - val_loss: 0.0021\n",
      "Epoch 23/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0424 - val_loss: 0.0019\n",
      "Epoch 24/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0377 - val_loss: 0.0017\n",
      "Epoch 25/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0339 - val_loss: 0.0014\n",
      "Epoch 26/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0342 - val_loss: 0.0013\n",
      "Epoch 27/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0346 - val_loss: 0.0012\n",
      "Epoch 28/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0340 - val_loss: 0.0010\n",
      "Epoch 29/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0316 - val_loss: 9.1771e-04\n",
      "Epoch 30/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0289 - val_loss: 7.9798e-04\n",
      "Epoch 31/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0270 - val_loss: 6.9267e-04\n",
      "Epoch 32/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0256 - val_loss: 6.3869e-04\n",
      "Epoch 33/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0285 - val_loss: 6.0732e-04\n",
      "Epoch 34/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0274 - val_loss: 5.2833e-04\n",
      "Epoch 35/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0279 - val_loss: 4.6925e-04\n",
      "Epoch 36/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0261 - val_loss: 4.5312e-04\n",
      "Epoch 37/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0261 - val_loss: 4.0178e-04\n",
      "Epoch 38/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0241 - val_loss: 3.6435e-04\n",
      "Epoch 39/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0258 - val_loss: 3.5447e-04\n",
      "Epoch 40/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0253 - val_loss: 3.0365e-04\n",
      "Epoch 41/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0240 - val_loss: 2.7910e-04\n",
      "Epoch 42/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0223 - val_loss: 2.6149e-04\n",
      "Epoch 43/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0211 - val_loss: 2.3099e-04\n",
      "Epoch 44/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0241 - val_loss: 2.1961e-04\n",
      "Epoch 45/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0200 - val_loss: 1.9552e-04\n",
      "Epoch 46/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0193 - val_loss: 1.7379e-04\n",
      "Epoch 47/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0213 - val_loss: 1.7537e-04\n",
      "Epoch 48/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0207 - val_loss: 1.5950e-04\n",
      "Epoch 49/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0240 - val_loss: 1.5839e-04\n",
      "Epoch 50/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0230 - val_loss: 1.5205e-04\n",
      "Epoch 51/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0210 - val_loss: 1.3008e-04\n",
      "Epoch 52/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0202 - val_loss: 1.1161e-04\n",
      "Epoch 53/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0199 - val_loss: 1.0295e-04\n",
      "Epoch 54/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0195 - val_loss: 1.0355e-04\n",
      "Epoch 55/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0162 - val_loss: 9.2911e-05\n",
      "Epoch 56/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0182 - val_loss: 7.7595e-05\n",
      "Epoch 57/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0182 - val_loss: 8.1062e-05\n",
      "Epoch 58/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0200 - val_loss: 7.7550e-05\n",
      "Epoch 59/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0208 - val_loss: 7.4720e-05\n",
      "Epoch 60/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0198 - val_loss: 7.3043e-05\n",
      "Epoch 61/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0158 - val_loss: 6.3008e-05\n",
      "Epoch 62/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0164 - val_loss: 5.6921e-05\n",
      "Epoch 63/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0178 - val_loss: 5.1257e-05\n",
      "Epoch 64/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0179 - val_loss: 5.6333e-05\n",
      "Epoch 65/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0193 - val_loss: 5.2624e-05\n",
      "Epoch 66/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0166 - val_loss: 4.5909e-05\n",
      "Epoch 67/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0149 - val_loss: 4.4134e-05\n",
      "Epoch 68/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0179 - val_loss: 3.4967e-05\n",
      "Epoch 69/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0163 - val_loss: 3.4862e-05\n",
      "Epoch 70/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0159 - val_loss: 3.8279e-05\n",
      "Epoch 71/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0175 - val_loss: 3.1486e-05\n",
      "Epoch 72/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0196 - val_loss: 3.1465e-05\n",
      "Epoch 73/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0166 - val_loss: 3.2294e-05\n",
      "Epoch 74/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0161 - val_loss: 2.9511e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0159 - val_loss: 2.5882e-05\n",
      "Epoch 76/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0156 - val_loss: 2.6698e-05\n",
      "Epoch 77/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0174 - val_loss: 2.6164e-05\n",
      "Epoch 78/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0188 - val_loss: 2.3976e-05\n",
      "Epoch 79/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0170 - val_loss: 2.1041e-05\n",
      "Epoch 80/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0184 - val_loss: 2.0629e-05\n",
      "Epoch 81/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0173 - val_loss: 2.0157e-05\n",
      "Epoch 82/1000\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0149 - val_loss: 1.7019e-05\n",
      "Epoch 83/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0152 - val_loss: 1.5698e-05\n",
      "Epoch 84/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0169 - val_loss: 1.4696e-05\n",
      "Epoch 85/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0171 - val_loss: 1.5627e-05\n",
      "Epoch 86/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0172 - val_loss: 1.4676e-05\n",
      "Epoch 87/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0160 - val_loss: 1.5327e-05\n",
      "Epoch 88/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0183 - val_loss: 1.3375e-05\n",
      "Epoch 89/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0166 - val_loss: 1.2903e-05\n",
      "Epoch 90/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0127 - val_loss: 1.1437e-05\n",
      "Epoch 91/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0135 - val_loss: 9.8604e-06\n",
      "Epoch 92/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0156 - val_loss: 1.1604e-05\n",
      "Epoch 93/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0167 - val_loss: 1.1055e-05\n",
      "Epoch 94/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0128 - val_loss: 9.9744e-06\n",
      "Epoch 95/1000\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.0145 - val_loss: 8.5395e-06\n",
      "Epoch 96/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0138 - val_loss: 8.0155e-06\n",
      "Epoch 97/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0123 - val_loss: 8.0665e-06\n",
      "Epoch 98/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0131 - val_loss: 7.2648e-06\n",
      "Epoch 99/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0128 - val_loss: 6.6102e-06\n",
      "Epoch 100/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0140 - val_loss: 6.5573e-06\n",
      "Epoch 101/1000\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.0168 - val_loss: 5.9881e-06\n",
      "Epoch 102/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0156 - val_loss: 7.1824e-06\n",
      "Epoch 103/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0177 - val_loss: 7.0451e-06\n",
      "Epoch 104/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0143 - val_loss: 6.1952e-06\n",
      "Epoch 105/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0137 - val_loss: 6.2464e-06\n",
      "Epoch 106/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0143 - val_loss: 5.1996e-06\n",
      "Epoch 107/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0128 - val_loss: 4.6824e-06\n",
      "Epoch 108/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0136 - val_loss: 4.3266e-06\n",
      "Epoch 109/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0148 - val_loss: 4.9627e-06\n",
      "Epoch 110/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0150 - val_loss: 4.8175e-06\n",
      "Epoch 111/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0167 - val_loss: 4.4822e-06\n",
      "Epoch 112/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0163 - val_loss: 4.6283e-06\n",
      "Epoch 113/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0144 - val_loss: 4.7238e-06\n",
      "Epoch 114/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0159 - val_loss: 6.8849e-06\n",
      "Epoch 115/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0165 - val_loss: 4.4062e-06\n",
      "Epoch 116/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0166 - val_loss: 3.4734e-06\n",
      "Epoch 117/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0187 - val_loss: 3.8987e-06\n",
      "Epoch 118/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0161 - val_loss: 3.9425e-06\n",
      "Epoch 119/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0132 - val_loss: 3.3947e-06\n",
      "Epoch 120/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0146 - val_loss: 3.4574e-06\n",
      "Epoch 121/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0113 - val_loss: 2.7211e-06\n",
      "Epoch 122/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0136 - val_loss: 3.2216e-06\n",
      "Epoch 123/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0158 - val_loss: 3.2433e-06\n",
      "Epoch 124/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0117 - val_loss: 2.5020e-06\n",
      "Epoch 125/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0132 - val_loss: 2.8043e-06\n",
      "Epoch 126/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0127 - val_loss: 3.0791e-06\n",
      "Epoch 127/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0135 - val_loss: 2.4200e-06\n",
      "Epoch 128/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0123 - val_loss: 2.1136e-06\n",
      "Epoch 129/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0121 - val_loss: 2.3356e-06\n",
      "Epoch 130/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0138 - val_loss: 2.5774e-06\n",
      "Epoch 131/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0123 - val_loss: 2.4413e-06\n",
      "Epoch 132/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0139 - val_loss: 2.1012e-06\n",
      "Epoch 133/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0132 - val_loss: 2.4071e-06\n",
      "Epoch 134/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0152 - val_loss: 1.8679e-06\n",
      "Epoch 135/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0159 - val_loss: 2.1334e-06\n",
      "Epoch 136/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0121 - val_loss: 1.6785e-06\n",
      "Epoch 137/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0126 - val_loss: 2.1000e-06\n",
      "Epoch 138/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0157 - val_loss: 1.8221e-06\n",
      "Epoch 139/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0135 - val_loss: 1.5209e-06\n",
      "Epoch 140/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0136 - val_loss: 1.5300e-06\n",
      "Epoch 141/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0153 - val_loss: 1.5787e-06\n",
      "Epoch 142/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0143 - val_loss: 1.3165e-06\n",
      "Epoch 143/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0161 - val_loss: 1.3602e-06\n",
      "Epoch 144/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0177 - val_loss: 1.5831e-06\n",
      "Epoch 145/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0138 - val_loss: 1.2690e-06\n",
      "Epoch 146/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0141 - val_loss: 1.4290e-06\n",
      "Epoch 147/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0123 - val_loss: 1.2379e-06\n",
      "Epoch 148/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0109 - val_loss: 9.5804e-07\n",
      "Epoch 149/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0133 - val_loss: 8.7992e-07\n",
      "Epoch 150/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0135 - val_loss: 9.1251e-07\n",
      "Epoch 151/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0123 - val_loss: 9.9301e-07\n",
      "Epoch 152/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0127 - val_loss: 1.3513e-06\n",
      "Epoch 153/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0137 - val_loss: 1.2677e-06\n",
      "Epoch 154/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0117 - val_loss: 9.0384e-07\n",
      "Epoch 155/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0121 - val_loss: 9.1537e-07\n",
      "Epoch 156/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0148 - val_loss: 1.1295e-06\n",
      "Epoch 157/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0156 - val_loss: 7.9878e-07\n",
      "Epoch 158/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0127 - val_loss: 7.9497e-07\n",
      "Epoch 159/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0112 - val_loss: 8.8747e-07\n",
      "Epoch 160/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0139 - val_loss: 8.0856e-07\n",
      "Epoch 161/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0121 - val_loss: 7.3886e-07\n",
      "Epoch 162/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0102 - val_loss: 6.3387e-07\n",
      "Epoch 163/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0136 - val_loss: 7.4609e-07\n",
      "Epoch 164/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0141 - val_loss: 6.9388e-07\n",
      "Epoch 165/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0148 - val_loss: 1.0162e-06\n",
      "Epoch 166/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0148 - val_loss: 6.8911e-07\n",
      "Epoch 167/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0125 - val_loss: 5.7689e-07\n",
      "Epoch 168/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0139 - val_loss: 6.6002e-07\n",
      "Epoch 169/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0113 - val_loss: 7.3099e-07\n",
      "Epoch 170/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0152 - val_loss: 8.0196e-07\n",
      "Epoch 171/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0171 - val_loss: 7.0564e-07\n",
      "Epoch 172/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0121 - val_loss: 5.8746e-07\n",
      "Epoch 173/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0160 - val_loss: 6.3499e-07\n",
      "Epoch 174/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0127 - val_loss: 5.3056e-07\n",
      "Epoch 175/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0138 - val_loss: 4.7557e-07\n",
      "Epoch 176/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0125 - val_loss: 5.2969e-07\n",
      "Epoch 177/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0162 - val_loss: 5.9430e-07\n",
      "Epoch 178/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0161 - val_loss: 6.5620e-07\n",
      "Epoch 179/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0136 - val_loss: 4.6341e-07\n",
      "Epoch 180/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0126 - val_loss: 4.6945e-07\n",
      "Epoch 181/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0138 - val_loss: 4.0412e-07\n",
      "Epoch 182/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0124 - val_loss: 3.8520e-07\n",
      "Epoch 183/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0149 - val_loss: 4.2375e-07\n",
      "Epoch 184/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0135 - val_loss: 3.9641e-07\n",
      "Epoch 185/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0148 - val_loss: 6.4476e-07\n",
      "Epoch 186/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0161 - val_loss: 4.7278e-07\n",
      "Epoch 187/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0137 - val_loss: 3.8560e-07\n",
      "Epoch 188/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0150 - val_loss: 3.9593e-07\n",
      "Epoch 189/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0146 - val_loss: 3.1670e-07\n",
      "Epoch 190/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0140 - val_loss: 3.2473e-07\n",
      "Epoch 191/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0096 - val_loss: 2.3564e-07\n",
      "Epoch 192/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0113 - val_loss: 2.6019e-07\n",
      "Epoch 193/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0116 - val_loss: 2.7728e-07\n",
      "Epoch 194/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0125 - val_loss: 3.0732e-07\n",
      "Epoch 195/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0117 - val_loss: 2.6298e-07\n",
      "Epoch 196/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0097 - val_loss: 2.7625e-07\n",
      "Epoch 197/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0121 - val_loss: 3.2163e-07\n",
      "Epoch 198/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0132 - val_loss: 2.8706e-07\n",
      "Epoch 199/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0113 - val_loss: 2.7347e-07\n",
      "Epoch 200/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0142 - val_loss: 2.3897e-07\n",
      "Epoch 201/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0108 - val_loss: 2.3739e-07\n",
      "Epoch 202/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0114 - val_loss: 2.5741e-07\n",
      "Epoch 203/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0115 - val_loss: 2.8825e-07\n",
      "Epoch 204/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0126 - val_loss: 2.1942e-07\n",
      "Epoch 205/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0107 - val_loss: 2.3532e-07\n",
      "Epoch 206/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0107 - val_loss: 2.5916e-07\n",
      "Epoch 207/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0110 - val_loss: 2.3150e-07\n",
      "Epoch 208/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0113 - val_loss: 1.8295e-07\n",
      "Epoch 209/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0129 - val_loss: 2.1450e-07\n",
      "Epoch 210/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0119 - val_loss: 2.0321e-07\n",
      "Epoch 211/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0111 - val_loss: 1.8414e-07\n",
      "Epoch 212/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0087 - val_loss: 2.3246e-07\n",
      "Epoch 213/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0135 - val_loss: 2.4350e-07\n",
      "Epoch 214/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0109 - val_loss: 1.5752e-07\n",
      "Epoch 215/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0122 - val_loss: 1.2906e-07\n",
      "Epoch 216/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0110 - val_loss: 1.3685e-07\n",
      "Epoch 217/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0131 - val_loss: 1.5656e-07\n",
      "Epoch 218/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0116 - val_loss: 1.4989e-07\n",
      "Epoch 219/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0096 - val_loss: 1.2581e-07\n",
      "Epoch 220/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0086 - val_loss: 1.3145e-07\n",
      "Epoch 221/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0080 - val_loss: 1.3638e-07\n",
      "Epoch 222/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0117 - val_loss: 1.5028e-07\n",
      "Epoch 223/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0090 - val_loss: 1.2469e-07\n",
      "Epoch 224/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0126 - val_loss: 1.4281e-07\n",
      "Epoch 225/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0095 - val_loss: 1.4464e-07\n",
      "Epoch 226/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0119 - val_loss: 1.2279e-07\n",
      "Epoch 227/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0109 - val_loss: 1.1730e-07\n",
      "Epoch 228/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0132 - val_loss: 1.2279e-07\n",
      "Epoch 229/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0116 - val_loss: 1.1635e-07\n",
      "Epoch 230/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0104 - val_loss: 1.0490e-07\n",
      "Epoch 231/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0135 - val_loss: 9.6401e-08\n",
      "Epoch 232/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0155 - val_loss: 2.0099e-07\n",
      "Epoch 233/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0125 - val_loss: 1.6419e-07\n",
      "Epoch 234/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0146 - val_loss: 1.7730e-07\n",
      "Epoch 235/1000\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.0162 - val_loss: 2.0774e-07\n",
      "Epoch 236/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0149 - val_loss: 1.3653e-07\n",
      "Epoch 237/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0144 - val_loss: 1.2080e-07\n",
      "Epoch 238/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0135 - val_loss: 1.2612e-07\n",
      "Epoch 239/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0133 - val_loss: 8.1460e-08\n",
      "Epoch 240/1000\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0110 - val_loss: 8.5592e-08\n",
      "Epoch 241/1000\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0116 - val_loss: 2.9230e-07\n",
      "Epoch 242/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0078 - val_loss: 8.9169e-08\n",
      "Epoch 243/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0152 - val_loss: 9.0758e-08\n",
      "Epoch 244/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0118 - val_loss: 1.0848e-07\n",
      "Epoch 245/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0136 - val_loss: 1.0252e-07\n",
      "Epoch 246/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0111 - val_loss: 1.6880e-07\n",
      "Epoch 247/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0156 - val_loss: 1.0848e-07\n",
      "Epoch 248/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0127 - val_loss: 1.4249e-07\n",
      "Epoch 249/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0159 - val_loss: 1.1945e-07\n",
      "Epoch 250/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0118 - val_loss: 1.1587e-07\n",
      "Epoch 251/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0157 - val_loss: 8.0506e-08\n",
      "Epoch 252/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0120 - val_loss: 2.8292e-07\n",
      "Epoch 253/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0176 - val_loss: 1.1500e-07\n",
      "Epoch 254/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0133 - val_loss: 1.2279e-07\n",
      "Epoch 255/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0124 - val_loss: 1.1770e-07\n",
      "Epoch 256/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0133 - val_loss: 1.7865e-07\n",
      "Epoch 257/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0119 - val_loss: 9.2188e-08\n",
      "Epoch 258/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0129 - val_loss: 8.0824e-08\n",
      "Epoch 259/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0103 - val_loss: 6.4611e-08\n",
      "Epoch 260/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0127 - val_loss: 9.3142e-08\n",
      "Epoch 261/1000\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0090 - val_loss: 9.2665e-08\n",
      "Epoch 262/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0094 - val_loss: 6.4532e-08\n",
      "Epoch 263/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0102 - val_loss: 8.3367e-08\n",
      "Epoch 264/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0094 - val_loss: 6.7155e-08\n",
      "Epoch 265/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0133 - val_loss: 9.2347e-08\n",
      "Epoch 266/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0127 - val_loss: 6.1274e-08\n",
      "Epoch 267/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0108 - val_loss: 9.3381e-08\n",
      "Epoch 268/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0124 - val_loss: 7.0333e-08\n",
      "Epoch 269/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0128 - val_loss: 7.3433e-08\n",
      "Epoch 270/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0122 - val_loss: 8.7976e-08\n",
      "Epoch 271/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0131 - val_loss: 1.0029e-07\n",
      "Epoch 272/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0115 - val_loss: 7.2559e-08\n",
      "Epoch 273/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0119 - val_loss: 6.1035e-08\n",
      "Epoch 274/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0141 - val_loss: 6.5724e-08\n",
      "Epoch 275/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0113 - val_loss: 9.3381e-08\n",
      "Epoch 276/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0116 - val_loss: 6.3817e-08\n",
      "Epoch 277/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0116 - val_loss: 6.9300e-08\n",
      "Epoch 278/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0131 - val_loss: 8.9089e-08\n",
      "Epoch 279/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0142 - val_loss: 8.7817e-08\n",
      "Epoch 280/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0124 - val_loss: 7.4943e-08\n",
      "Epoch 281/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0119 - val_loss: 5.8969e-08\n",
      "Epoch 282/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0122 - val_loss: 1.0594e-07\n",
      "Epoch 283/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0162 - val_loss: 9.0440e-08\n",
      "Epoch 284/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0118 - val_loss: 8.8771e-08\n",
      "Epoch 285/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0136 - val_loss: 4.8558e-08\n",
      "Epoch 286/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0121 - val_loss: 5.1181e-08\n",
      "Epoch 287/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0111 - val_loss: 9.0281e-08\n",
      "Epoch 288/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0157 - val_loss: 5.3962e-08\n",
      "Epoch 289/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0115 - val_loss: 5.9128e-08\n",
      "Epoch 290/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0097 - val_loss: 4.5061e-08\n",
      "Epoch 291/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0108 - val_loss: 4.1326e-08\n",
      "Epoch 292/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0119 - val_loss: 4.0372e-08\n",
      "Epoch 293/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0118 - val_loss: 2.0663e-08\n",
      "Epoch 294/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0144 - val_loss: 3.1074e-08\n",
      "Epoch 295/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0101 - val_loss: 2.9564e-08\n",
      "Epoch 296/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0138 - val_loss: 3.0518e-08\n",
      "Epoch 297/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0117 - val_loss: 3.6875e-08\n",
      "Epoch 298/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0110 - val_loss: 4.4664e-08\n",
      "Epoch 299/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0133 - val_loss: 5.0147e-08\n",
      "Epoch 300/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0114 - val_loss: 4.6651e-08\n",
      "Epoch 301/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0112 - val_loss: 5.0863e-08\n",
      "Epoch 302/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0127 - val_loss: 3.8147e-08\n",
      "Epoch 303/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0125 - val_loss: 2.8610e-08\n",
      "Epoch 304/1000\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0118 - val_loss: 3.2028e-08\n",
      "Epoch 305/1000\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0121 - val_loss: 5.8492e-08\n",
      "Epoch 306/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0129 - val_loss: 6.1909e-08\n",
      "Epoch 307/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0133 - val_loss: 6.4294e-08\n",
      "Epoch 308/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0159 - val_loss: 8.4162e-08\n",
      "Epoch 309/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0156 - val_loss: 8.3685e-08\n",
      "Epoch 310/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0126 - val_loss: 4.9194e-08\n",
      "Epoch 311/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0105 - val_loss: 3.5365e-08\n",
      "Epoch 312/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0102 - val_loss: 3.7670e-08\n",
      "Epoch 313/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0125 - val_loss: 3.9419e-08\n",
      "Epoch 314/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0135 - val_loss: 3.8147e-08\n",
      "Epoch 315/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0095 - val_loss: 2.4557e-08\n",
      "Epoch 316/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0110 - val_loss: 2.6782e-08\n",
      "Epoch 317/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0124 - val_loss: 2.8690e-08\n",
      "Epoch 318/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0123 - val_loss: 3.7750e-08\n",
      "Epoch 319/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0114 - val_loss: 4.0134e-08\n",
      "Epoch 320/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0130 - val_loss: 4.4187e-08\n",
      "Epoch 321/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0103 - val_loss: 3.8942e-08\n",
      "Epoch 322/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0095 - val_loss: 3.1710e-08\n",
      "Epoch 323/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0106 - val_loss: 2.8451e-08\n",
      "Epoch 324/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0151 - val_loss: 2.4080e-08\n",
      "Epoch 325/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0128 - val_loss: 3.2743e-08\n",
      "Epoch 326/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0116 - val_loss: 2.7895e-08\n",
      "Epoch 327/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0112 - val_loss: 2.7895e-08\n",
      "Epoch 328/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0107 - val_loss: 2.9325e-08\n",
      "Epoch 329/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0103 - val_loss: 2.1060e-08\n",
      "Epoch 330/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0140 - val_loss: 2.8610e-08\n",
      "Epoch 331/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0107 - val_loss: 1.7246e-08\n",
      "Epoch 332/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0117 - val_loss: 2.3524e-08\n",
      "Epoch 333/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0118 - val_loss: 1.4146e-08\n",
      "Epoch 334/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0098 - val_loss: 1.7166e-08\n",
      "Epoch 335/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0075 - val_loss: 2.3206e-08\n",
      "Epoch 336/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0116 - val_loss: 2.6862e-08\n",
      "Epoch 337/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0138 - val_loss: 1.8438e-08\n",
      "Epoch 338/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0090 - val_loss: 1.7881e-08\n",
      "Epoch 339/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0102 - val_loss: 1.4544e-08\n",
      "Epoch 340/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0138 - val_loss: 2.6464e-08\n",
      "Epoch 341/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0125 - val_loss: 2.1776e-08\n",
      "Epoch 342/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0107 - val_loss: 2.5431e-08\n",
      "Epoch 343/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0123 - val_loss: 2.7021e-08\n",
      "Epoch 344/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0108 - val_loss: 2.3683e-08\n",
      "Epoch 345/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0108 - val_loss: 1.4623e-08\n",
      "Epoch 346/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0150 - val_loss: 1.5497e-08\n",
      "Epoch 347/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0109 - val_loss: 1.8676e-08\n",
      "Epoch 348/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0123 - val_loss: 1.1603e-08\n",
      "Epoch 349/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0114 - val_loss: 1.6610e-08\n",
      "Epoch 350/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0117 - val_loss: 2.1299e-08\n",
      "Epoch 351/1000\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0112 - val_loss: 1.9789e-08\n",
      "Epoch 352/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0090 - val_loss: 1.2159e-08\n",
      "Epoch 353/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0132 - val_loss: 1.2239e-08\n",
      "Epoch 354/1000\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0107 - val_loss: 1.3590e-08\n",
      "Epoch 355/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0121 - val_loss: 1.6530e-08\n",
      "Epoch 356/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0125 - val_loss: 3.0279e-08\n",
      "Epoch 357/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0122 - val_loss: 1.3192e-08\n",
      "Epoch 358/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0126 - val_loss: 1.0649e-08\n",
      "Epoch 359/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0113 - val_loss: 2.0663e-08\n",
      "Epoch 360/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0080 - val_loss: 1.6848e-08\n",
      "Epoch 361/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0119 - val_loss: 1.5497e-08\n",
      "Epoch 362/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0086 - val_loss: 7.1526e-09\n",
      "Epoch 363/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0110 - val_loss: 2.0822e-08\n",
      "Epoch 364/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0108 - val_loss: 1.6689e-08\n",
      "Epoch 365/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0122 - val_loss: 1.1603e-08\n",
      "Epoch 366/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0098 - val_loss: 1.3987e-08\n",
      "Epoch 367/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0130 - val_loss: 8.1062e-09\n",
      "Epoch 368/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0105 - val_loss: 1.3431e-08\n",
      "Epoch 369/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0116 - val_loss: 1.2716e-08\n",
      "Epoch 370/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0121 - val_loss: 1.9550e-08\n",
      "Epoch 371/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0104 - val_loss: 1.4782e-08\n",
      "Epoch 372/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0111 - val_loss: 1.7563e-08\n",
      "Epoch 373/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0138 - val_loss: 1.0888e-08\n",
      "Epoch 374/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0159 - val_loss: 2.0425e-08\n",
      "Epoch 375/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0132 - val_loss: 2.4557e-08\n",
      "Epoch 376/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0113 - val_loss: 1.2398e-08\n",
      "Epoch 377/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0113 - val_loss: 1.3351e-08\n",
      "Epoch 378/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0100 - val_loss: 2.2173e-08\n",
      "Epoch 379/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0130 - val_loss: 1.9073e-08\n",
      "Epoch 380/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0104 - val_loss: 2.4954e-08\n",
      "Epoch 381/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0129 - val_loss: 2.8690e-08\n",
      "Epoch 382/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0115 - val_loss: 1.7405e-08\n",
      "Epoch 383/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0123 - val_loss: 1.0331e-08\n",
      "Epoch 384/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0101 - val_loss: 7.6294e-09\n",
      "Epoch 385/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0118 - val_loss: 1.7166e-08\n",
      "Epoch 386/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0113 - val_loss: 6.3578e-09\n",
      "Epoch 387/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0142 - val_loss: 1.0490e-08\n",
      "Epoch 388/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0155 - val_loss: 1.3351e-08\n",
      "Epoch 389/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0131 - val_loss: 1.0490e-08\n",
      "Epoch 390/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0117 - val_loss: 1.8438e-08\n",
      "Epoch 391/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0107 - val_loss: 2.7180e-08\n",
      "Epoch 392/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0116 - val_loss: 2.4001e-08\n",
      "Epoch 393/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0117 - val_loss: 3.2504e-08\n",
      "Epoch 394/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0121 - val_loss: 1.2954e-08\n",
      "Epoch 395/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0150 - val_loss: 7.5499e-09\n",
      "Epoch 396/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0110 - val_loss: 9.9341e-09\n",
      "Epoch 397/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0140 - val_loss: 1.6451e-08\n",
      "Epoch 398/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0082 - val_loss: 8.1857e-09\n",
      "Epoch 399/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0094 - val_loss: 6.8347e-09\n",
      "Epoch 400/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0080 - val_loss: 4.6094e-09\n",
      "Epoch 401/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0093 - val_loss: 8.5036e-09\n",
      "Epoch 402/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0091 - val_loss: 1.2159e-08\n",
      "Epoch 403/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0097 - val_loss: 8.2652e-09\n",
      "Epoch 404/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0116 - val_loss: 1.2477e-08\n",
      "Epoch 405/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0087 - val_loss: 4.9273e-09\n",
      "Epoch 406/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0103 - val_loss: 1.4385e-08\n",
      "Epoch 407/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0101 - val_loss: 1.1921e-08\n",
      "Epoch 408/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0089 - val_loss: 1.1047e-08\n",
      "Epoch 409/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0117 - val_loss: 5.4042e-09\n",
      "Epoch 410/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0115 - val_loss: 1.1126e-08\n",
      "Epoch 411/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0101 - val_loss: 3.7352e-09\n",
      "Epoch 412/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0094 - val_loss: 2.3127e-08\n",
      "Epoch 413/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0146 - val_loss: 4.0531e-09\n",
      "Epoch 414/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0110 - val_loss: 7.6294e-09\n",
      "Epoch 415/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0133 - val_loss: 8.1062e-09\n",
      "Epoch 416/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0143 - val_loss: 1.0729e-08\n",
      "Epoch 417/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0149 - val_loss: 3.4968e-09\n",
      "Epoch 418/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0106 - val_loss: 7.9473e-09\n",
      "Epoch 419/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0114 - val_loss: 1.3590e-08\n",
      "Epoch 420/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0158 - val_loss: 3.3776e-08\n",
      "Epoch 421/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0112 - val_loss: 2.3921e-08\n",
      "Epoch 422/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0098 - val_loss: 1.1285e-08\n",
      "Epoch 423/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0114 - val_loss: 6.6757e-09\n",
      "Epoch 424/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0135 - val_loss: 1.5418e-08\n",
      "Epoch 425/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0103 - val_loss: 7.7089e-09\n",
      "Epoch 426/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0135 - val_loss: 7.6294e-09\n",
      "Epoch 427/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0105 - val_loss: 6.6757e-09\n",
      "Epoch 428/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0102 - val_loss: 5.8015e-09\n",
      "Epoch 429/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0099 - val_loss: 6.8347e-09\n",
      "Epoch 430/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0090 - val_loss: 4.6094e-09\n",
      "Epoch 431/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0077 - val_loss: 2.3047e-09\n",
      "Epoch 432/1000\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0092 - val_loss: 4.9273e-09\n",
      "Epoch 433/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0090 - val_loss: 7.3910e-09\n",
      "Epoch 434/1000\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0113 - val_loss: 1.2636e-08\n",
      "Epoch 435/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0130 - val_loss: 1.4385e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x232c62ac2b0>"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(patience=100, restore_best_weights=True, monitor=\"loss\")\n",
    "autoencoder.fit(data, data,\n",
    "                epochs=1000,\n",
    "                batch_size=300,\n",
    "                validation_data=(val_data, val_data),\n",
    "               callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#autoencoder.save('4_7_symbol_autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder_loaded = load_model('4_7_symbol_autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_signal, encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(n_channel,))\n",
    "\n",
    "deco = autoencoder.layers[-2](encoded_input)\n",
    "deco = autoencoder.layers[-1](deco)\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, deco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 45000\n",
    "test_label = np.random.randint(M,size=N)\n",
    "test_data = []\n",
    "\n",
    "for i in test_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    test_data.append(temp)\n",
    "    \n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 87\n"
     ]
    }
   ],
   "source": [
    "temp_test = 6\n",
    "print (test_data[temp_test][test_label[temp_test]],test_label[temp_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x232c61b1198>"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: -4 BER: 0.7868888888888889\n",
      "SNR: -3.5 BER: 0.7576444444444445\n",
      "SNR: -3.0 BER: 0.7214888888888888\n",
      "SNR: -2.5 BER: 0.6813333333333333\n",
      "SNR: -2.0 BER: 0.6387111111111111\n",
      "SNR: -1.5 BER: 0.5897555555555556\n",
      "SNR: -1.0 BER: 0.5437111111111111\n",
      "SNR: -0.5 BER: 0.4838\n",
      "SNR: 0.0 BER: 0.4261333333333333\n",
      "SNR: 0.5 BER: 0.372\n",
      "SNR: 1.0 BER: 0.31244444444444447\n",
      "SNR: 1.5 BER: 0.2637111111111111\n",
      "SNR: 2.0 BER: 0.2098888888888889\n",
      "SNR: 2.5 BER: 0.16262222222222222\n",
      "SNR: 3.0 BER: 0.12304444444444444\n",
      "SNR: 3.5 BER: 0.09033333333333333\n",
      "SNR: 4.0 BER: 0.06246666666666666\n",
      "SNR: 4.5 BER: 0.04084444444444445\n",
      "SNR: 5.0 BER: 0.02668888888888889\n",
      "SNR: 5.5 BER: 0.01671111111111111\n",
      "SNR: 6.0 BER: 0.008977777777777777\n",
      "SNR: 6.5 BER: 0.0054444444444444445\n",
      "SNR: 7.0 BER: 0.0024222222222222223\n",
      "SNR: 7.5 BER: 0.0013333333333333333\n"
     ]
    }
   ],
   "source": [
    "ber = [None]*len(EbNodB_range)\n",
    "for n in range(0,len(EbNodB_range)):\n",
    "    EbNo=10.0**(EbNodB_range[n]/10.0)\n",
    "    noise_std = np.sqrt(1/(2*R*EbNo))\n",
    "    noise_mean = 0\n",
    "    no_errors = 0\n",
    "    nn = N\n",
    "    noise = noise_std * np.random.randn(nn,n_channel)\n",
    "    encoded_signal = encoder.predict(test_data) \n",
    "    final_signal = encoded_signal + noise\n",
    "    pred_final_signal =  decoder.predict(final_signal)\n",
    "    pred_output = np.argmax(pred_final_signal,axis=1)\n",
    "    no_errors = (pred_output != test_label)\n",
    "    no_errors =  no_errors.astype(int).sum()\n",
    "    ber[n] = no_errors / nn \n",
    "    print ('SNR:',EbNodB_range[n],'BER:',ber[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(EbNodB_range, ber, 'bo',label='Autoencoder({}, {})'.format(n_channel, k))\n",
    "#plt.plot(list(EbNodB_range), ber_theory, 'ro-',label='BPSK BER')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('SNR Range')\n",
    "plt.ylabel('Block Error Rate')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right',ncol = 1)\n",
    "plt.savefig('AutoEncoder_{}_{}_BER.png'.format(n_channel, k))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot = []\n",
    "for i in range(0,M):\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    scatter_plot.append(encoder.predict(np.expand_dims(temp,axis=0)))\n",
    "scatter_plot = 1.5/2*np.array(scatter_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2048 into shape (256,2,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-520-465c0cfa730e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# ploting constellation diagram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscatter_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscatter_plot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscatter_plot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscatter_plot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#plt.axis((-2.5,2.5,-2.5,2.5))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2048 into shape (256,2,1)"
     ]
    }
   ],
   "source": [
    "# ploting constellation diagram\n",
    "import matplotlib.pyplot as plt\n",
    "scatter_plot = scatter_plot.reshape(M,2,1)\n",
    "plt.scatter(scatter_plot[:,0, :],scatter_plot[:,1, :])\n",
    "#plt.axis((-2.5,2.5,-2.5,2.5))\n",
    "plt.grid()\n",
    "plt.xlabel('I Axis')\n",
    "plt.ylabel('Q Axis')\n",
    "plt.savefig('AutoEncoder_{}_{}_Constellation.png'.format(n_channel, k))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
